{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Techniques Explained\n",
    "\n",
    "\n",
    "## Table of Content\n",
    "\n",
    "\n",
    "&emsp;&emsp;[1 Concepts & Strategies](#chp1) <br>\n",
    "&emsp;&emsp;&emsp;&emsp;1.1 Core concepts <br>\n",
    "&emsp;&emsp;&emsp;&emsp;1.2 Strategies <br>\n",
    "&emsp;&emsp;2 Classifiers <br>\n",
    "&emsp;&emsp;&emsp;&emsp;2.1 logistic regression  <br>\n",
    "&emsp;&emsp;&emsp;&emsp;2.2 K nearest neighbor classifier (kNN) <br>\n",
    "&emsp;&emsp;&emsp;&emsp;2.3 K-mean <br>\n",
    "&emsp;&emsp;&emsp;&emsp;2.4 Expectation Maximization (EM) <br>\n",
    "&emsp;&emsp;&emsp;&emsp;2.5 Density-based spatial clustering of application with noises (DBSCAN) <br>\n",
    "&emsp;&emsp;&emsp;&emsp;2.6 Support vector machine classifier (SVM-classifier) <br>\n",
    "&emsp;&emsp;&emsp;&emsp;2.7 Decision tree classifier (DTree-classifier) <br>\n",
    "&emsp;&emsp;&emsp;&emsp;2.8 Random forest classifier (RForest-classifier) <br>\n",
    "&emsp;&emsp;&emsp;&emsp;2.9 Gradient boost tree classifier (GBoost classifier) <br>\n",
    "&emsp;&emsp;&emsp;&emsp;2.10 Extreme boost tree classifier (XGBoost classifier) <br>\n",
    "&emsp;&emsp;&emsp;&emsp;2.11 Adam boost tree classifier (ADBoost classifier) <br>\n",
    "&emsp;&emsp;3 Regressors <br>\n",
    "&emsp;&emsp;&emsp;&emsp;3.1 Linear regression <br>\n",
    "&emsp;&emsp;&emsp;&emsp;3.2 Linear mixed model <br>\n",
    "&emsp;&emsp;&emsp;&emsp;3.3 Penalized regression <br>\n",
    "&emsp;&emsp;&emsp;&emsp;3.4 Support vector machine regression (SVM-regression) <br>\n",
    "&emsp;&emsp;&emsp;&emsp;3.5 Decision tree regression (DTree-regression) <br>\n",
    "&emsp;&emsp;&emsp;&emsp;3.6 Random forest regression (RForest-regression) <br>\n",
    "&emsp;&emsp;&emsp;&emsp;3.7 Gradient boost tree regression (GBoost regression) <br>\n",
    "&emsp;&emsp;&emsp;&emsp;3.8 Extreme boost tree regression (XGBoost regression) <br>\n",
    "&emsp;&emsp;&emsp;&emsp;3.9 Adam boost tree regression (ADBoost regression) <br>\n",
    "&emsp;&emsp;4 Data Preprocessing and Feature Engineering <br>\n",
    "&emsp;&emsp;&emsp;&emsp;4.1 tabular data <br>\n",
    "&emsp;&emsp;&emsp;&emsp;4.2 images <br>\n",
    "&emsp;&emsp;&emsp;&emsp;4.3 time-series <br>\n",
    "&emsp;&emsp;&emsp;&emsp;4.4 text <br>\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='chp1'></a>\n",
    "# Concepts & Strategies\n",
    "\n",
    "> - [General concepts](#sec1-1) \n",
    "- [Classification metrics](#sec1-2) \n",
    "- [Time series data](#sec1-3) \n",
    "- [Spatial data](#sec1-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec1-1'></a>\n",
    "* ***What is the difference between supervised learning and unsupervised learning?***\n",
    "\n",
    "> Supervised learning uses the training datasets with both input features and targets to train the model that maps the input space to the output space. On the other hands, the unsupervised learning doesn't require targets. It trains only on input features and tries to extract the patterns from input space, and such pattern can be either used to group data or engineer features.\n",
    "\n",
    "* ***What is the bias-variance trade off?***\n",
    "\n",
    "> The bias is the error (unexplained residual) of your model due to the oversimplification of model hypotheses. A biased model tends to under-fit the data, resulting a bad prediction accuracy. In contrast, the variance is introduced by the increasing complexity of the model. A very complex model hypothesis tends to over-fit the training data and learns the noises; therefore, its parameter can vary drastically while fitting different random samples from the population. This is how we say the model falls to the variance side and could not be generalized. In practices, data scientist often needs to tweak the complexity in order to balance the trade off between bias and variance, or in order words, to balance the under-fitting and over-fitting.\n",
    "\n",
    "* ***What is selection bias?***\n",
    "\n",
    "> Selection bias occurs while the proper sampling (randomization) is not achieved, resulting the obtained samples are not representative of the population.\n",
    "\n",
    "* ***What is selection bias?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec1-2'></a>\n",
    "* ***What is confusion matrix?***\n",
    "\n",
    "> Confusion matrix is a 4 by 4 table that contains 4 elements from a binary classification result. The columns are the predicted classes and rows are actual classes. The element (0, 0) corresponds to the number of samples that is predicted as positive and is also actual positive, called **true positive (TP)**. (0, 1) corresponds to the number of samples that is actual positive but predicted as negative, called **false negative (FN)**. (1, 0) is the number of negative samples that has been predicted as positive, called **false positive (FP)**. (1, 1) is the number of negative that is correctly assigned to negative, which is **true negative (TN)**. <br>\n",
    "<img src=\"pics/1-2.png\" align=\"center\"/>\n",
    "[Figure 1-2. Confusion matrix and associated binary classification metrics](https://en.wikipedia.org/wiki/F1_score)\n",
    "\n",
    "> There are many metrics associated with binary classification and all related to confusion matrix, they are: <br>\n",
    "1. **Error rate**: (FP + FN) / Total_Samples\n",
    "2. **Overall accuracy**: (TP + TN) / Total_Samples\n",
    "3. **Sensitivity (recall or true positive rate)**: TP / Total_Positives\n",
    "4. **Fall-out (false positive rate)**: FP / Total_Negatives\n",
    "5. **Specificity (true negative rate)**: TN / Total_Negatives\n",
    "6. **Precision (positive predicted value)**: TP / (TP + FP)\n",
    "7. **F-score (harmonic mean of precision and recall)**: (1 + $\\beta^2$) $\\times$ (Precision $\\times$ Recall) / ($\\beta^2 \\times$  Precision + Recall). While $\\beta$ is 1, we call it F1-score, which balances the precision and recall. While $\\beta$ is between 0 and 1, it assigns more weight on precision and biases towards false positive responses. When $\\beta$ is above 1, F-score would weight on recall being favorable on more true positive responses.\n",
    "\n",
    "> In the field of image processing or remote sensing, confusion matrix is extended to deal with multi-label classification. Different to the original confusion matrix, people normally define columns as the references, and the rows as the predictions. <br>\n",
    "<img src=\"pics/1-3.png\" height=300 width=300 align=\"center\"/>\n",
    "[Figure 1-3. An example of confusion matrix used for image classification](http://gis.humboldt.edu/OLM/Courses/GSP_216_Online/lesson6-2/metrics.html)\n",
    "\n",
    "> The related metrics are:\n",
    "1. **Overall accuracy and error (overall)**: Total_Correct_Pred / Total_Samples\n",
    "2. **Omission error (per class)**: Incorrect_Classified_Ref / Total_Ref\n",
    "3. **Commission error (per class)**: Incorrect_Classified_Pred / Total_Pred\n",
    "4. **Producer's accuracy (per class)**: Correct_Classified_Ref / Total_Ref = 1 - Omission_Error\n",
    "5. **User's accuracy (per class)**: Correct_Classified_Pred / Total_Pred = 1 - Commission_Error\n",
    "6. **Kappa (both overall and conditional to each class)** is generated from statistical test that compares the classification outcome to random. Kappa index is ranging from -1 to 1 (or 0 to 1 sometime), where 0 (or 0.5) indicates random classification, below 0 (or 0.5) indicates worser than random, and above 0 (or 0.5) indicates better than random. **Conditional Kappa** for class $i$ is given as:  \n",
    "\\begin{equation*}\n",
    "Kappa(i) = \\frac{P_{i,i}(y=i) - P_i(y=i) P(y=i)} {P_i(y=i) -  P_i(y=i) P(y=i) }\n",
    "\\end{equation*}\n",
    " <br> where $P_{i,i}(y=i)$ is the probability of agreement, i.e. the number of correctly classified samples of class $i$ divided by the total number of references;\n",
    " <br> $P_i(y=i)$ is the probability of true $i$ having classified the point as $i$, i.e. the total number of correctly classified class $i$ divided by the total number of points being predicted as $i$;\n",
    " <br> and $P(y=i)$ is the probability of samples being classified as $i$, i.e. the total number of predicts of class $i$ divided by the total number of references. \n",
    "<br> **Overall Kappa** is given as:\n",
    "\\begin{equation*}\n",
    "Kappa = \\frac{\\sum{P_{i,i}(y=i)} - \\sum{P_i(y=i)P(y=i)} } {1 - \\sum{P_i(y=i) P(y=i)} }\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n",
    "* ***What is ROC curve?***\n",
    "\n",
    "> ROC (Receiver Operating Characteristic) curve is a graphical representation of the contrast between true positive rate and false positive rate at various thresholds. It is often used as a proxy of trade off between true positive rate and false positive rate of the model prediction. <br>\n",
    "<img src=\"pics/1-4.png\" height=300 width=300 align=\"center\"/>\n",
    "[Figure 1-4. An example of ROC curve for different models](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec1-3'></a>\n",
    "* ***What are the common questions answered by the time-series analysis?***\n",
    "> 1. Are the patterns occurred on current day related to the priors (autocorrelation)?\n",
    "  2. Can predict future based on history (prediction)?\n",
    "  3. Are certain patterns related to certain dates (cross-autocorrelation)?\n",
    "\n",
    "* ***What are the steps for time series analysis?***\n",
    "> Following OSEMN model, the steps of time series analysis could be:\n",
    "  1. Data preprocessing\n",
    "  2. EDA (statistics, outliers detection (scatter and box plot), autocorrelation check (ACF and PACF))\n",
    "  3. Model identification (error dependency (Durbin-Waston test), stationarity (Dickey-Fuller test)). If need an autoregressive error model. If needs to difference data with particular differencing order to obtain stationarity.\n",
    "  4. Model estimation (maximum likelihood)\n",
    "  5. Diagnostic checking (check if autocorrelation of time ordered residuals)\n",
    "  \n",
    "* ***Time series statistical modeling techniques***\n",
    "\n",
    "> 1. Autocorrelation Function (ACF): Pearson correlation between time lags. <br>\n",
    "  2. Patial Autocorrelation Function (PACF): Pearson correlation between time lags without other linear dependencies from random variables within the time lag. <br>\n",
    "  3. Autoregressive model (AR):\n",
    "  4. Regression with autoregressive error\n",
    "  5. Autoregressive moving average (ARMA(p, q)):\n",
    "  5. Autoregressive integrated moving average (ARIMA(d, p, q)):\n",
    "  \n",
    "\n",
    "* ***What cross-validation technique would you use on time-series dataset?***\n",
    "> Instead of using k-fold, one should be aware of that the time-series is not randomly distributed. It is inherently ordered in the chronological manner. In the cross-validation, you should use techniques like forward chaining, e.g. <br> <br>\n",
    "    train 1: [0, 0, 0, 0, 1] test 1: [2] <br>\n",
    "    train 2: [0, 0, 0, 1, 2] test 2: [3] <br>\n",
    "    train 3: [0, 0, 1, 2, 3] test 3: [4] <br>\n",
    "    train 4: [0, 1, 2, 3, 4] test 4: [5] <br>\n",
    "    ... <br> <br>\n",
    "where you would like to train on past data, and test on forward-facing data\n",
    "\n",
    "* ***How to deal with NAs in time-series (python)?***\n",
    "> While working with pandas, first it needs to transform the date column into index value of the **DataFrame** and make sure it is as a **date object** to be understood by Python. <br>\n",
    "For filling **NaN**, there are many techniques that can used, i.e. <br> \n",
    ">\n",
    "```Python\n",
    "# simple fill\n",
    "df.assign(Fill_Mean = df.Target.fillna(df.Target.mean())) # fill mean\n",
    "df.assign(Fill_Median = df.Target.fillna(df.Target.median())) # fill median\n",
    "# rolling statistics\n",
    "df.assign(Rolling_Mean = df.Target.fillna(df.Target.rolling(window, min_periods).mean()) # rolling mean\n",
    "df.assign(Rolling_Median = df.Target.fillna(df.Target.rolling(window, min_periods).median()) # rolling median\n",
    "# interpolation\n",
    "df = df.assign(InterpolateLinear    =df.target.interpolate(method='linear'))\n",
    "df = df.assign(InterpolateTime      =df.target.interpolate(method='time'))\n",
    "df = df.assign(InterpolateQuadratic =df.target.interpolate(method='quadratic'))\n",
    "df = df.assign(InterpolateCubic     =df.target.interpolate(method='cubic'))\n",
    "df = df.assign(InterpolateSLinear   =df.target.interpolate(method='slinear'))\n",
    "df = df.assign(InterpolateAkima     =df.target.interpolate(method='akima'))\n",
    "df = df.assign(InterpolatePoly5     =df.target.interpolate(method='polynomial', order=5)) \n",
    "df = df.assign(InterpolatePoly7     =df.target.interpolate(method='polynomial', order=7))\n",
    "df = df.assign(InterpolateSpline3   =df.target.interpolate(method='spline',     order=3))\n",
    "df = df.assign(InterpolateSpline4   =df.target.interpolate(method='spline',     order=4))\n",
    "df = df.assign(InterpolateSpline5   =df.target.interpolate(method='spline',     order=5))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sec1-4'></a>\n",
    "* ***What is spatial index?***\n",
    "> Spatial index is a data structure that allows efficiently accessing the spatial data, as well as efficient spatial operation - range query, spatial join and kNN.\n",
    "\n",
    "* ***What is Minimum Bounding Rectangle?***\n",
    "> In order to reduce the cost of calculating complex shape of spatial object during query transversal, people commonly use **Minimum Bounding Rectangle (MBR)** as an approximation of object's geometry. In 2D space, the MBR is consist of two points $(x_min, y_min)$ and $(x_max, y_max)$. While performing spatial operation on a collection of indexed spatial object, we first use MBRs to test the relationship to filter the objects. Then only perform operation on the subset of the spatial objects.\n",
    "\n",
    "* ***What are the common data structures in different spatial database?***\n",
    "> 1. Space-driven structure: partitioning the underlaying 2D space into cells (grids), and then mapping the MBRs on top of the them. Applications are IBM DB2, MS SQL Server, and ESRI GeoDataBase <br>\n",
    "  2. Data-driven structure: directly organized by partition of the collection of spatial objects. Application are Oracle, and other open-source database, PostGIS and MySQL\n",
    "\n",
    "* ***What are the popular spatial index techniques used in space-driven structure?***\n",
    "> 1. Fixed grid index (spatial grids at different spatial scales)\n",
    "  2. Quadtree index (Leaf node with no child or internal node with four children)\n",
    "  3. KD-tree (each node represents one axis (x or y) and splits based on whether there is an object with the coordinate greater or less than itself in such axis)\n",
    "  4. Geohash (geocoding spatial point as short string to be used in web URLs)\n",
    "  \n",
    "* ***What are the popular spatial index techniques used in data-driven structure?***\n",
    "> 1. R-tree (hierarchical MBR structure based on heuristic optimization of area of MBRs in each node)\n",
    "  2. R-tree variants\n",
    "  \n",
    ">> Tips: <br>\n",
    "1. [GeoSpark](https://github.com/DataSystemsLab/GeoSpark) an in-memory cluster computing framework for processing large-scale spatial dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
