{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20200213024621-0000\nKERNEL_ID = e301e143-1949-4927-a959-acec2e29b89c\n--2020-02-13 02:46:24--  https://github.com/IBM/coursera/raw/master/hmp.parquet\nResolving github.com (github.com)... 140.82.113.4\nConnecting to github.com (github.com)|140.82.113.4|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/IBM/coursera/master/hmp.parquet [following]\n--2020-02-13 02:46:24--  https://raw.githubusercontent.com/IBM/coursera/master/hmp.parquet\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 199.232.8.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|199.232.8.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 932997 (911K) [application/octet-stream]\nSaving to: 'hmp.parquet'\n\n100%[======================================>] 932,997     --.-K/s   in 0.02s   \n\n2020-02-13 02:46:24 (45.1 MB/s) - 'hmp.parquet' saved [932997/932997]\n\n+---+---+---+--------------------+-----------+\n|  x|  y|  z|              source|      class|\n+---+---+---+--------------------+-----------+\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n| 21| 52| 34|Accelerometer-201...|Brush_teeth|\n| 22| 51| 34|Accelerometer-201...|Brush_teeth|\n| 20| 50| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 34|Accelerometer-201...|Brush_teeth|\n| 22| 50| 34|Accelerometer-201...|Brush_teeth|\n| 22| 51| 35|Accelerometer-201...|Brush_teeth|\n+---+---+---+--------------------+-----------+\nonly showing top 10 rows\n\n"
                }
            ],
            "source": "# get parquet\n\n!wget https://github.com/IBM/coursera/raw/master/hmp.parquet\n    \ndf = spark.read.parquet('hmp.parquet')\n\ndf.show(10)"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": "# register a corresponding sql table\ndf.createOrReplaceTempView('df')"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+---+---+--------------------+-----------+-----------+--------------+----------------+--------------------+\n|  x|  y|  z|              source|      class|class_index| encoded_class|        features|   features_rescaled|\n+---+---+---+--------------------+-----------+-----------+--------------+----------------+--------------------+\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|        6.0|(13,[6],[1.0])|[22.0,49.0,35.0]|[0.34920634920634...|\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|        6.0|(13,[6],[1.0])|[22.0,49.0,35.0]|[0.34920634920634...|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|        6.0|(13,[6],[1.0])|[22.0,52.0,35.0]|[0.34920634920634...|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|        6.0|(13,[6],[1.0])|[22.0,52.0,35.0]|[0.34920634920634...|\n| 21| 52| 34|Accelerometer-201...|Brush_teeth|        6.0|(13,[6],[1.0])|[21.0,52.0,34.0]|[0.33333333333333...|\n| 22| 51| 34|Accelerometer-201...|Brush_teeth|        6.0|(13,[6],[1.0])|[22.0,51.0,34.0]|[0.34920634920634...|\n| 20| 50| 35|Accelerometer-201...|Brush_teeth|        6.0|(13,[6],[1.0])|[20.0,50.0,35.0]|[0.31746031746031...|\n| 22| 52| 34|Accelerometer-201...|Brush_teeth|        6.0|(13,[6],[1.0])|[22.0,52.0,34.0]|[0.34920634920634...|\n| 22| 50| 34|Accelerometer-201...|Brush_teeth|        6.0|(13,[6],[1.0])|[22.0,50.0,34.0]|[0.34920634920634...|\n| 22| 51| 35|Accelerometer-201...|Brush_teeth|        6.0|(13,[6],[1.0])|[22.0,51.0,35.0]|[0.34920634920634...|\n+---+---+---+--------------------+-----------+-----------+--------------+----------------+--------------------+\nonly showing top 10 rows\n\n"
                }
            ],
            "source": "# create a data preprocessing pipeline\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, MinMaxScaler\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml import Pipeline\n\nindexer = StringIndexer(inputCol='class', outputCol='class_index')\nencoder = OneHotEncoder(inputCol='class_index', outputCol='encoded_class')\nvectorizer = VectorAssembler(inputCols=['x','y','z'], outputCol='features')\nrescaler = MinMaxScaler(inputCol='features', outputCol='features_rescaled')\n\npipeline = Pipeline(stages=[indexer, encoder, vectorizer, rescaler]).fit(df)\npreprocessed_df = pipeline.transform(df)\npreprocessed_df.show(10)"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+---+---+--------------------+-----------+-----------+--------------+----------------+--------------------+\n|  x|  y|  z|              source|      class|class_index| encoded_class|        features|   features_rescaled|\n+---+---+---+--------------------+-----------+-----------+--------------+----------------+--------------------+\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|        6.0|(13,[6],[1.0])|[22.0,49.0,35.0]|[0.34920634920634...|\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|        6.0|(13,[6],[1.0])|[22.0,49.0,35.0]|[0.34920634920634...|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|        6.0|(13,[6],[1.0])|[22.0,52.0,35.0]|[0.34920634920634...|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|        6.0|(13,[6],[1.0])|[22.0,52.0,35.0]|[0.34920634920634...|\n| 21| 52| 34|Accelerometer-201...|Brush_teeth|        6.0|(13,[6],[1.0])|[21.0,52.0,34.0]|[0.33333333333333...|\n| 22| 51| 34|Accelerometer-201...|Brush_teeth|        6.0|(13,[6],[1.0])|[22.0,51.0,34.0]|[0.34920634920634...|\n| 20| 50| 35|Accelerometer-201...|Brush_teeth|        6.0|(13,[6],[1.0])|[20.0,50.0,35.0]|[0.31746031746031...|\n| 22| 52| 34|Accelerometer-201...|Brush_teeth|        6.0|(13,[6],[1.0])|[22.0,52.0,34.0]|[0.34920634920634...|\n| 22| 50| 34|Accelerometer-201...|Brush_teeth|        6.0|(13,[6],[1.0])|[22.0,50.0,34.0]|[0.34920634920634...|\n| 22| 51| 35|Accelerometer-201...|Brush_teeth|        6.0|(13,[6],[1.0])|[22.0,51.0,35.0]|[0.34920634920634...|\n+---+---+---+--------------------+-----------+-----------+--------------+----------------+--------------------+\nonly showing top 10 rows\n\n"
                }
            ],
            "source": "# create a data preprocessing pipeline\nfrom pyspark.ml.feature import OneHotEncoderEstimator\n\nindexer = StringIndexer(inputCol='class', outputCol='class_index')\nencoder = OneHotEncoderEstimator(inputCols=['class_index'], outputCols=['encoded_class'])\nvectorizer = VectorAssembler(inputCols=['x','y','z'], outputCol='features')\nrescaler = MinMaxScaler(inputCol='features', outputCol='features_rescaled')\n\npipeline = Pipeline(stages=[indexer, encoder, vectorizer, rescaler]).fit(df)\npreprocessed_df = pipeline.transform(df)\npreprocessed_df.show(10)"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+---+---+--------------------+-----------+-----------+--------------+----------------+--------------------+--------------------+\n|  x|  y|  z|              source|      class|class_index| encoded_class|        features|       features_norm|   features_rescaled|\n+---+---+---+--------------------+-----------+-----------+--------------+----------------+--------------------+--------------------+\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|        6.0|(13,[6],[1.0])|[22.0,49.0,35.0]|[0.34316403829308...|[0.35689598697470...|\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|        6.0|(13,[6],[1.0])|[22.0,49.0,35.0]|[0.34316403829308...|[0.35689598697470...|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|        6.0|(13,[6],[1.0])|[22.0,52.0,35.0]|[0.33117360613218...|[0.34442574929594...|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|        6.0|(13,[6],[1.0])|[22.0,52.0,35.0]|[0.33117360613218...|[0.34442574929594...|\n| 21| 52| 34|Accelerometer-201...|Brush_teeth|        6.0|(13,[6],[1.0])|[21.0,52.0,34.0]|[0.32020976616922...|[0.33302318361897...|\n| 22| 51| 34|Accelerometer-201...|Brush_teeth|        6.0|(13,[6],[1.0])|[22.0,51.0,34.0]|[0.33782247905081...|[0.35134068150844...|\n| 20| 50| 35|Accelerometer-201...|Brush_teeth|        6.0|(13,[6],[1.0])|[20.0,50.0,35.0]|[0.31139957766460...|[0.32386044926769...|\n| 22| 52| 34|Accelerometer-201...|Brush_teeth|        6.0|(13,[6],[1.0])|[22.0,52.0,34.0]|[0.33379342096892...|[0.34715039785323...|\n| 22| 50| 34|Accelerometer-201...|Brush_teeth|        6.0|(13,[6],[1.0])|[22.0,50.0,34.0]|[0.34191842968811...|[0.35560053447138...|\n| 22| 51| 35|Accelerometer-201...|Brush_teeth|        6.0|(13,[6],[1.0])|[22.0,51.0,35.0]|[0.33510742122295...|[0.34851697874522...|\n+---+---+---+--------------------+-----------+-----------+--------------+----------------+--------------------+--------------------+\nonly showing top 10 rows\n\n"
                }
            ],
            "source": "from pyspark.ml.feature import Normalizer\n\nindexer = StringIndexer(inputCol='class', outputCol='class_index')\nencoder = OneHotEncoderEstimator(inputCols=['class_index'], outputCols=['encoded_class'])\nvectorizer = VectorAssembler(inputCols=['x','y','z'], outputCol='features')\nnormalizer = Normalizer(inputCol='features', outputCol='features_norm')\nrescaler = MinMaxScaler(inputCol='features_norm', outputCol='features_rescaled')\n\npipeline = Pipeline(stages=[indexer, encoder, vectorizer, normalizer, rescaler]).fit(df)\npreprocessed_df = pipeline.transform(df)\npreprocessed_df.show(10)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6 with Spark",
            "language": "python3",
            "name": "python36"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}