{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20200214032343-0000\nKERNEL_ID = 02823108-e91d-4d83-8dce-d9e23e8dc688\n--2020-02-14 03:23:45--  https://github.com/IBM/coursera/raw/master/hmp.parquet\nResolving github.com (github.com)... 192.30.253.112\nConnecting to github.com (github.com)|192.30.253.112|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/IBM/coursera/master/hmp.parquet [following]\n--2020-02-14 03:23:46--  https://raw.githubusercontent.com/IBM/coursera/master/hmp.parquet\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 199.232.8.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|199.232.8.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 932997 (911K) [application/octet-stream]\nSaving to: 'hmp.parquet'\n\n100%[======================================>] 932,997     --.-K/s   in 0.03s   \n\n2020-02-14 03:23:46 (27.7 MB/s) - 'hmp.parquet' saved [932997/932997]\n\n+---+---+---+--------------------+-----------+\n|  x|  y|  z|              source|      class|\n+---+---+---+--------------------+-----------+\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n| 21| 52| 34|Accelerometer-201...|Brush_teeth|\n+---+---+---+--------------------+-----------+\nonly showing top 5 rows\n\n"
                }
            ],
            "source": "# download the file containing the data in PARQUET format\n!wget https://github.com/IBM/coursera/raw/master/hmp.parquet\n\n# create a dataframe out of it\ndf = spark.read.parquet('hmp.parquet')\n\ndf.createOrReplaceTempView('df')\n\ndf.show(5)"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": "df_energy = spark.sql(\"\"\"\n    select sqrt(sum(x*x) + sum(y*y) + sum(z*z)) as label, class from df group by class\n\"\"\")"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": "df_energy.createOrReplaceTempView('df_energy')"
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+---+---+-----------------+-----------+\n|  x|  y|  z|            label|      class|\n+---+---+---+-----------------+-----------+\n| 22| 49| 35|11785.39634462923|Brush_teeth|\n| 22| 49| 35|11785.39634462923|Brush_teeth|\n| 22| 52| 35|11785.39634462923|Brush_teeth|\n| 22| 52| 35|11785.39634462923|Brush_teeth|\n| 21| 52| 34|11785.39634462923|Brush_teeth|\n+---+---+---+-----------------+-----------+\nonly showing top 5 rows\n\n"
                }
            ],
            "source": "df_join = spark.sql(\"\"\"\n    \n    select x, y, z, label, df.class from df inner join df_energy on df.class=df_energy.class\n\n\"\"\")\ndf_join.show(5)"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": "from pyspark.ml.feature import VectorAssembler, Normalizer\nfrom pyspark.ml import Pipeline\n\nvectorAssembler = VectorAssembler(inputCols=['x','y','z'], outputCol='features')\nnormalizer = Normalizer(inputCol='features', outputCol='features_norm', p=1.0)"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": "from pyspark.ml.regression import LinearRegression\n\nlr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "<pyspark.ml.regression.LinearRegressionTrainingSummary at 0x7f8aa420a3c8>"
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "pipeline = Pipeline(stages=[vectorAssembler, normalizer, lr])\nmodel = pipeline.fit(df_join)\nprediction=model.transform(df_join)\n\nmodel.stages[2].summary"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "<pyspark.ml.regression.LinearRegressionTrainingSummary at 0x7f8a802ef668>"
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "model.stages[2].summary"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "0.03259100556263628"
                    },
                    "execution_count": 15,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "model.stages[2].summary.r2"
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "+---+---+---+-----------+\n|  x|  y|  z|      class|\n+---+---+---+-----------+\n| 22| 49| 35|Brush_teeth|\n| 22| 49| 35|Brush_teeth|\n| 22| 52| 35|Brush_teeth|\n| 22| 52| 35|Brush_teeth|\n| 21| 52| 34|Brush_teeth|\n+---+---+---+-----------+\nonly showing top 5 rows\n\n"
                }
            ],
            "source": "df_join = df_join.drop('label')\ndf_join.show(5)"
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [],
            "source": "(df_train, df_test) = df_join.randomSplit([0.8,0.2])\n"
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [],
            "source": "from pyspark.ml.classification import LogisticRegression\n\nlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)"
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [],
            "source": "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n\nindexer = StringIndexer(inputCol='class', outputCol='label')\nencoder = OneHotEncoder(inputCol='class_index', outputCol='class_encoded')\n\npipeline = Pipeline(stages=[indexer, vectorAssembler, normalizer, lr])\nmodel = pipeline.fit(df_train)"
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [],
            "source": "prediction = model.transform(df_train)"
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [],
            "source": "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\nevaluation = MulticlassClassificationEvaluator().setMetricName('accuracy').setLabelCol('label').setPredictionCol('prediction')"
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "0.20685364701145537"
                    },
                    "execution_count": 31,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "evaluation.evaluate(prediction)"
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "0.2055918091809181"
                    },
                    "execution_count": 32,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "evaluation.evaluate(model.transform(df_test))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6 with Spark",
            "language": "python3",
            "name": "python36"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}